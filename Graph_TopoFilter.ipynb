{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArturBudniak/Graph_TopoFilter/blob/main/Graph_TopoFilter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jrZP9r-b-uZ"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmCKT-h9NMTK",
        "outputId": "30a3e27e-b372-48b8-b41f-a6e80e4d40fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting networkx==2.3\n",
            "  Downloading networkx-2.3.zip (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from networkx==2.3) (4.4.2)\n",
            "Building wheels for collected packages: networkx\n",
            "  Building wheel for networkx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for networkx: filename=networkx-2.3-py2.py3-none-any.whl size=1556009 sha256=7b669fe4bc202f1fd8e7c68e1679748ef4c97c43f0b3c8c6c5ad2840bafbb292\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/62/9e/0ed2d25fd4f5761e2d19568cda0c32716556dfa682e65ecf64\n",
            "Successfully built networkx\n",
            "Installing collected packages: networkx\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.0\n",
            "    Uninstalling networkx-3.0:\n",
            "      Successfully uninstalled networkx-3.0\n",
            "Successfully installed networkx-2.3\n"
          ]
        }
      ],
      "source": [
        "pip install networkx==2.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3PqRVjEycMlb"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3wP7zETGcAJZ"
      },
      "outputs": [],
      "source": [
        "# Code to read csv file from Drive in Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7i0o00vWhORf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FqYNPtkATJG",
        "outputId": "1573a57b-2fd8-45e3-c928-35798c8a07a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.8/dist-packages (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-hub) (1.22.4)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-hub) (3.19.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.8/dist-packages (4.8.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (0.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (2.25.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (8.1.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (1.12.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (2.2.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (5.4.8)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (0.1.8)\n",
            "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (1.22.4)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (5.12.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (4.64.1)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets) (3.19.6)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.8/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (4.5.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.8/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (3.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets) (4.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from promise->tensorflow-datasets) (1.15.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.58.0)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-hub\n",
        "!pip install tensorflow-datasets\n",
        "\n",
        "# A dependency of the preprocessing for BERT inputs\n",
        "!pip install -q -U tensorflow-text\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9jbYW5CN4nCw"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from random import randint\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nvrnW5yV0kFc"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "j0hJ7OVHwkBY"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import kneighbors_graph\n",
        "from statistics import mean "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTOMAdxL142J"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yH5jbVSjmBw2"
      },
      "outputs": [],
      "source": [
        "# predict top n most probable components with statistics\n",
        "\n",
        "def predict_top_n_components (text, model_name, n, failure_ratio):\n",
        "\n",
        "  y_proba = model_name.predict([text])\n",
        "\n",
        "  if failure_ratio == True:\n",
        "    for i in range(0,number_of_labels):\n",
        "      y_proba[0][i] = y_proba[0][i] * components_failure_rate[\"Rate\"].loc[y_test_set.columns[i]]\n",
        "\n",
        "\n",
        "  df_top_n_components = pd.DataFrame(y_proba[0], index =df_cleaned.columns[-15:], columns =['probability'])\n",
        "\n",
        "\n",
        "  return(df_top_n_components.sort_values([\"probability\"], ascending=False)[:n])#.index.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BsZB_5eumK39"
      },
      "outputs": [],
      "source": [
        "# create a new neural network for text classification\n",
        "\n",
        "def define_model_NLP(number_of_1st_hidden_layer_nodes, number_of_2nd_hidden_layer_nodes):\n",
        "  embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
        "  hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=False)\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(hub_layer)\n",
        "  model.add(tf.keras.layers.Dense(number_of_1st_hidden_layer_nodes, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dense(number_of_2nd_hidden_layer_nodes, activation='relu'))   \n",
        "  model.add(tf.keras.layers.Dense(number_of_labels, activation = 'softmax')) \n",
        "\n",
        "  model.compile(optimizer='adam', \n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "               )\n",
        "  return(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RenYW127AAA5"
      },
      "outputs": [],
      "source": [
        "# execute one epoch training of the neural network\n",
        "\n",
        "def train_model_NLP (model,data_set):\n",
        "  # split dataset into training and testing sets\n",
        "  train_set, test_set = train_test_split(data_set, test_size=0.2, random_state=42)\n",
        "  # split into X (features) and y (labels)\n",
        "  X_train_set = train_set[\"Customer Requested Comment\"]\n",
        "  y_train_set = train_set[train_set.columns[-number_of_labels:]]\n",
        "  X_test_set = test_set[\"Customer Requested Comment\"]\n",
        "  y_test_set = test_set[test_set.columns[-number_of_labels:]]\n",
        "  \n",
        "  history = model.fit(X_train_set, y_train_set,\n",
        "                    epochs=1,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_test_set, y_test_set),\n",
        "                    )\n",
        "  \n",
        "  return(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xyscugiPDhIz"
      },
      "outputs": [],
      "source": [
        "# get the hidden representation of points (one before last layer of the neural network)\n",
        "\n",
        "def get_latent_features(dataset, n):                                            # n=0 -> all layers, n=-1 -> without last layer, etc\n",
        "  model_without_last_layer = tf.keras.Sequential()\n",
        "  if n==0:\n",
        "      for layer in model.layers[:]:\n",
        "        model_without_last_layer.add(layer)\n",
        "  else:\n",
        "      for layer in model.layers[:n]:\n",
        "        model_without_last_layer.add(layer)\n",
        "    \n",
        "  model_without_last_layer.compile(optimizer='adam', loss='categorical_crossentropy');\n",
        "\n",
        "  return(model_without_last_layer.predict(dataset[\"Customer Requested Comment\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HBzuRH6i5OgC"
      },
      "outputs": [],
      "source": [
        "# construct kNN graph over hidden represenation of data points (taken from TopoFilter)\n",
        "\n",
        "def construct_G():\n",
        "\n",
        "  kNN_matrix = kneighbors_graph(x, k, mode='connectivity', include_self=False)\n",
        "\n",
        "  # create graph from kNN matrix\n",
        "  G = nx.from_scipy_sparse_matrix(kNN_matrix)\n",
        "\n",
        "  # assign attributes (labels) to nodes\n",
        "  label = S[\"Global Component Code Description\"]\n",
        "  label = label.to_list()\n",
        "\n",
        "  for node in range(0,len(G)):\n",
        "    G.nodes[node]['label'] = label[node]\n",
        "\n",
        "  return(G)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nsWZuONna0mT"
      },
      "outputs": [],
      "source": [
        "# for points belonging to class i create a subgraph of G with points only from that class; cut edges between class i points and other classes points (TopoFilter)\n",
        "\n",
        "def construct_Gi():\n",
        "  # remove nodes not belonging to given class i\n",
        "  selected_label = list_of_labels[i]\n",
        "  selected_data = dict( (n,d['label']) for n,d in G.nodes().items() if d['label'] == selected_label)\n",
        "  \n",
        "  #print(selected_data)\n",
        "  to_be_removed = G.nodes - list(selected_data.keys())\n",
        "  to_be_removed = list(to_be_removed)\n",
        "  Gi = G.copy()\n",
        "  for node in to_be_removed:\n",
        "    Gi.remove_node(node)\n",
        "  \n",
        "  return (Gi)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4LB77W6FItJN"
      },
      "outputs": [],
      "source": [
        "# for points beloning to class i create a subgraph of G with points only from that class, cut edges between [class i and its neighbor points (according to knowledge graph KG)] and other classes points \n",
        "\n",
        "def construct_Gi_k_hops(k):\n",
        "\n",
        "  # remove nodes not belonging to given class or its k-hops neighbors\n",
        "  selected_label = []\n",
        "  # root node:\n",
        "  selected_label.append(list_of_labels[i])\n",
        "\n",
        "  for node in appliance_G.nodes:\n",
        "  \n",
        "    target = node\n",
        "    source = selected_label[0]\n",
        "    \n",
        "    if nx.has_path(appliance_G, source, target):\n",
        "      distance = nx.shortest_path_length(appliance_G, source=source, target=target)\n",
        "      # if the distance < k hops, then add to the list of k-hops neighbors\n",
        "      if distance <= k:\n",
        "        #selected_label.append(source)\n",
        "        selected_label.append(target)\n",
        "  \n",
        "  selected_data = dict( (n,d['label']) for n,d in G.nodes().items() if d['label'] in selected_label)\n",
        "\n",
        "  to_be_removed = G.nodes - list(selected_data.keys())\n",
        "  to_be_removed = list(to_be_removed)\n",
        "  Gi = G.copy()\n",
        "  for node in to_be_removed:\n",
        "    Gi.remove_node(node)\n",
        "\n",
        "  return (Gi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MA-REY3GpIQB"
      },
      "outputs": [],
      "source": [
        "# select points of Gi or Gi_k_hops that belong to the largest connectoed component (samples expected to be clean)\n",
        "\n",
        "def construct_largest_connected_component():\n",
        "  \n",
        "  if len(Gi) > 0:\n",
        "    largest_cc = max(nx.connected_components(Gi), key=len)\n",
        "\n",
        "    # remove nodes not belonging to largest connected component\n",
        "    to_be_removed = Gi.nodes - largest_cc\n",
        "    to_be_removed = list(to_be_removed)\n",
        "    Qi = Gi.copy()\n",
        "    for node in to_be_removed:\n",
        "      Qi.remove_node(node)\n",
        "    \n",
        "    #remove nodes from other classes (if k-hop neighbor Gi)\n",
        "    to_be_removed = []\n",
        "    for node in Qi:\n",
        "      if Qi.nodes[node]['label'] != list_of_labels[i]:\n",
        "        to_be_removed.append(node)\n",
        "    for node in to_be_removed:\n",
        "      Qi.remove_node(node)\n",
        "\n",
        "    return(Qi)\n",
        "  else:\n",
        "    Qi = nx.Graph()\n",
        "    return(Qi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1fqCRnMdrFGm"
      },
      "outputs": [],
      "source": [
        "# add new samples (from lcc) to the clean data subset C\n",
        "\n",
        "def update_C():\n",
        "  if len(Qi) > 0:\n",
        "    C_new = pd.DataFrame(S.iloc[list(Qi.nodes)])\n",
        "  else:\n",
        "    C_new=[]\n",
        "  \n",
        "  return(C_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuBDPOjySF63"
      },
      "source": [
        "# Data load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iERktRCgf5lA"
      },
      "outputs": [],
      "source": [
        "# download service calls data set from Google Drive, not revealed\n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"1Nf4qVI0hWFP0CQSck_68GdR_QfWMYZQN\"})\n",
        "downloaded.GetContentFile('filename.csv')\n",
        "df = pd.read_csv('filename.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1KUXAug16f9o",
        "outputId": "48a03a1f-4777-41fb-8a45-12a5132d08a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                               Customer Requested Comment  \\\n",
              "500623                       GAS RANGE OVEN DOOR FELL OFF   \n",
              "755512                 CASH ON DELIVERY_OVEN NOT IGNITING   \n",
              "703461  THE CONVECTION FAN NOT WORKING AND SHUTS OVEN OFF   \n",
              "49907                           UNIT WONT IGNITE ON BAKE.   \n",
              "828860                                   OVEN NOT HEATING   \n",
              "\n",
              "            Global Component Code Description  \n",
              "500623    FASTENERS/CLIPS/SCREWS/BOLTS (9030)  \n",
              "755512  IGNITER ASSEMBLY (DRYER, OVEN) (5120)  \n",
              "703461           PERFORMANCE COMPLAINT (8220)  \n",
              "49907   IGNITER ASSEMBLY (DRYER, OVEN) (5120)  \n",
              "828860    CONTROL BOARD / SENSOR BOARD (5210)  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-235f0e86-7bd6-4541-91bd-8cfc7bb71d3c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Customer Requested Comment</th>\n",
              "      <th>Global Component Code Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>500623</th>\n",
              "      <td>GAS RANGE OVEN DOOR FELL OFF</td>\n",
              "      <td>FASTENERS/CLIPS/SCREWS/BOLTS (9030)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>755512</th>\n",
              "      <td>CASH ON DELIVERY_OVEN NOT IGNITING</td>\n",
              "      <td>IGNITER ASSEMBLY (DRYER, OVEN) (5120)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>703461</th>\n",
              "      <td>THE CONVECTION FAN NOT WORKING AND SHUTS OVEN OFF</td>\n",
              "      <td>PERFORMANCE COMPLAINT (8220)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49907</th>\n",
              "      <td>UNIT WONT IGNITE ON BAKE.</td>\n",
              "      <td>IGNITER ASSEMBLY (DRYER, OVEN) (5120)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>828860</th>\n",
              "      <td>OVEN NOT HEATING</td>\n",
              "      <td>CONTROL BOARD / SENSOR BOARD (5210)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-235f0e86-7bd6-4541-91bd-8cfc7bb71d3c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-235f0e86-7bd6-4541-91bd-8cfc7bb71d3c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-235f0e86-7bd6-4541-91bd-8cfc7bb71d3c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "df[[\"Customer Requested Comment\", \"Global Component Code Description\"]].sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqHHscwO0pHx"
      },
      "source": [
        "# Data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mS7yHfQR0rcJ"
      },
      "outputs": [],
      "source": [
        "# make a copy of dataframe read from csv file\n",
        "df_cleaned = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "32PuS3WF1wih"
      },
      "outputs": [],
      "source": [
        "# remove rows with missing values\n",
        "df_cleaned.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "J8_nPpTdTfot"
      },
      "outputs": [],
      "source": [
        "# reshuffle\n",
        "df_cleaned = shuffle(df_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2-EDs_ztEaUw"
      },
      "outputs": [],
      "source": [
        "# change to lowercase\n",
        "df_cleaned[\"Customer Requested Comment\"] = df_cleaned[\"Customer Requested Comment\"].str.lower()\n",
        "df_cleaned[\"Service Technician Comment\"] = df_cleaned[\"Service Technician Comment\"].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FVztKaWugPpC"
      },
      "outputs": [],
      "source": [
        "# remove special characters\n",
        "df_cleaned[\"Customer Requested Comment\"].replace(to_replace = '_',value = ' ',regex=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gCDVNRoGZDro"
      },
      "outputs": [],
      "source": [
        "# choose a model of the home appliace\n",
        "df_cleaned = df_cleaned[df_cleaned[\"DPL Platform Code\"] == \"WO\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "sgZbnjv9ayhe"
      },
      "outputs": [],
      "source": [
        "# remove multiple repairs assigned to the same ID number\n",
        "number_of_multiple_repairs = 0\n",
        "threshold = 2\n",
        "my_list = df_cleaned[\"Service Claim Id\"].value_counts()\n",
        "for i in my_list:\n",
        "  if i >= threshold:\n",
        "    number_of_multiple_repairs = number_of_multiple_repairs + 1\n",
        "\n",
        "# remove all duplicates\n",
        "df_cleaned.drop_duplicates(subset=['Service Claim Id'], inplace = True, keep = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "rOLUmXg68LXv"
      },
      "outputs": [],
      "source": [
        "# delete labels that have less than for example 100 rows\n",
        "\n",
        "threshold = 100  # Remove items less than or equal to threshold\n",
        "vc = df_cleaned[\"Global Component Code Description\"].value_counts()\n",
        "vals_to_remove = vc[vc <= threshold].index.values\n",
        "\n",
        "df_cleaned.drop(df_cleaned.loc[df_cleaned[\"Global Component Code Description\"].isin(vals_to_remove) ].index, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "JDKLCJEwGof0"
      },
      "outputs": [],
      "source": [
        "#remove labels which cannot be represented as components in graph representing the whole product\n",
        "\n",
        "list_of_labels_to_be_removed = [\"INSTRUCTION FOR USE (9910)\", \"CUSTOMER REFUSED REPAIR (9965)\",\"MISC. EXTERNAL PARTS/HARDWARE (8370)\",\"GENERAL SAFETY QUESTIONS (9975)\", \"CONNECTIVITY CUSTOMER INSTRUCT (8350)\", \"PERFORMANCE COMPLAINT (8220)\"]\n",
        "\n",
        "for i in list_of_labels_to_be_removed:\n",
        "  df_cleaned.drop(df_cleaned[df_cleaned[\"Global Component Code Description\"] == i].index, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_9lFjIHWqpj",
        "outputId": "bce458a4-01ed-4e18-f998-0576fc8c748d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of labels  46\n"
          ]
        }
      ],
      "source": [
        "# number of classes\n",
        "\n",
        "list_of_components = df_cleaned[\"Global Component Code Description\"].value_counts()\n",
        "i=0\n",
        "for w in list_of_components.index:\n",
        "  i=i+1\n",
        "\n",
        "number_of_labels = i\n",
        "print(\"number of labels \",number_of_labels)\n",
        "\n",
        "list_of_labels = df_cleaned[\"Global Component Code Description\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ZhpUXQNGV0oV"
      },
      "outputs": [],
      "source": [
        "# select only two columns X = Customer Requested Comment and Y = Global Component Code Description\n",
        "\n",
        "df_cleaned = df_cleaned[[\"Customer Requested Comment\", \"Global Component Code Description\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "wdREk1eFkcgP"
      },
      "outputs": [],
      "source": [
        "# encode categorical Y (Components)\n",
        "df_cleaned = pd.concat([df_cleaned, pd.get_dummies(df_cleaned[\"Global Component Code Description\"])], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "9r32iSld5vDY"
      },
      "outputs": [],
      "source": [
        "# assign labels to position in one-hot encoding\n",
        "\n",
        "values =  df_cleaned.columns.values[2:]\n",
        "keys = list(range(0, len(values)))\n",
        "dict_labels = dict(zip(keys, values))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_FmGIRPamEz"
      },
      "source": [
        "# Graph representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "krMMSgFJ7WiJ"
      },
      "outputs": [],
      "source": [
        "#read knowledge graph from file; the graph shows the home appliance construction (relationships between classes, not revealed)\n",
        "#https://drive.google.com/file/d/1Na4-HK78Zb5b9cJ1ME6TcgU1Jd4JcgB2/view?usp=share_link graphml\n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"1Na4-HK78Zb5b9cJ1ME6TcgU1Jd4JcgB2\"})\n",
        "downloaded.GetContentFile('appliance.graphml')\n",
        "appliance_G = nx.read_graphml(\"appliance.graphml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bwezUTIanX9"
      },
      "source": [
        "#Validate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fj6lTz4rJTEW"
      },
      "outputs": [],
      "source": [
        "list_of_manual_files = [\"11s1YfHiXmBGH8PQPwkznG9MPSV_4iolt\",\"1QgsbMb_U47R5Jn3YcQaBwkxbKWRYjvJt\",\"1yA6LPtrTIFKN9rDDug_Ur_dPRUwYcVoT\",\"1IMvwyxBZAdk4LHDd1_dqqDFnHXJCcT3r\", \"1iWMdB8LIl1AkTYrioHvyCAUS5K-GyrwW\", \"1b6A4cZb8fFz3m61t7vgzXkswg0Qx56bU\" ] #\"1R33d6JencbhfsgABTrE76gDM-dnCM_he\",\n",
        "list_of_manual_components = [\"HOOK/DOOR-LATCH (1310)\", \"LAMP (6520)\",\"HINGES (1880)\", \"THERMOSTAT (5550)\", \"MOTOR-FAN, CIRCULATION (4400)\", \"PRODUCT FUSE (5630)\"]\n",
        "\n",
        "#a part of the dataset for the evaluation\n",
        "fraction = 0.2\n",
        "print(\"fraction of the data set \",fraction)\n",
        "\n",
        "m = 15\n",
        "N = 20\n",
        "gamma = number_of_labels\n",
        "k = 10\n",
        "#zeta = 0.0\n",
        "\n",
        "#graph directed or undirected\n",
        "appliance_G = appliance_G.to_undirected()\n",
        "\n",
        "\n",
        "for k_hops in range(0,3):\n",
        "\n",
        "  ####################################################\n",
        "  #perform the TopoFilter / Graph TopoFilter algorithm\n",
        "  print(\"#########\")\n",
        "  print(\"k_hops \",k_hops)\n",
        "  S = shuffle(df_cleaned)\n",
        "  S=S.sample(frac = fraction)\n",
        "  S.reset_index(drop=True, inplace=True)\n",
        "  model = define_model_NLP(200,200)\n",
        "\n",
        "  #3. Initialize\n",
        "  C = pd.DataFrame(columns=[\"Customer Requested Comment\", \"Global Component Code Description\"])\n",
        "  S_hat = S\n",
        "\n",
        "  #4.\n",
        "  for t in range(1,N):\n",
        "\n",
        "    #5. Train network on S_hat\n",
        "    model = train_model_NLP(model, S_hat)\n",
        "\n",
        "    #6.\n",
        "    if t >= m:\n",
        "      #7. Extract feature vecotr x from training data S\n",
        "      x = get_latent_features(S,-1)\n",
        "      y = S[\"Global Component Code Description\"]\n",
        "      \n",
        "\n",
        "      #8. Compute k-NN graph G over x\n",
        "      G = construct_G()\n",
        "\n",
        "      #9.\n",
        "      for i in range (0,gamma):\n",
        "        Gi = construct_Gi_k_hops(k_hops)\n",
        "\n",
        "        #11. Compute the largest connected componet\n",
        "        Qi = construct_largest_connected_component()\n",
        "\n",
        "        #12. C ← C U Qi\n",
        "        C_add = update_C()\n",
        "        C=C.append(C_add)\n",
        "        \n",
        "      #13. end_for\n",
        "\n",
        "      #14. Find outliers O within C based on ζ-filtering; update C ← C\\O\n",
        "      #C = remove_outliers(C)\n",
        "\n",
        "      #15. S_hat ← C\n",
        "      S_hat = C\n",
        "    \n",
        "    #16. end_if\n",
        "\n",
        "  #17. end_for  \n",
        "\n",
        "  ###################################################\n",
        "  #validation for 6 files that were manually verified\n",
        "  list_of_F1_scores = []\n",
        "  list_of_sizes_of_clean_data_sets = []\n",
        "\n",
        "  for file_number in range (0,len(list_of_manual_files)):\n",
        "    \n",
        "    downloaded = drive.CreateFile({'id':list_of_manual_files[file_number]})\n",
        "    downloaded.GetContentFile('clean_data_set.csv')\n",
        "    clean_data_set = pd.read_csv('clean_data_set.csv')\n",
        "\n",
        "    #remove duplicates from clean data set \n",
        "    clean_data_set.drop_duplicates(subset = [\"Customer Requested Comment\"], inplace = True, keep = \"first\") \n",
        "\n",
        "    #remove rows that are not present in S (when fraction of S is set to be <1)\n",
        "    i = 0\n",
        "    S_subset = S[S[\"Global Component Code Description\"]==list_of_manual_components[file_number]]\n",
        "    while i < len(clean_data_set):\n",
        "      not_found = True\n",
        "      for j in range (0, len(S_subset)):\n",
        "        if clean_data_set[\"Customer Requested Comment\"].iloc[i] == S_subset[\"Customer Requested Comment\"].iloc[j]:\n",
        "          not_found = False\n",
        "      if not_found:\n",
        "        clean_data_set.drop(clean_data_set.iloc[i].name, inplace=True)\n",
        "        #print(\"usunieto wiersz \",i)\n",
        "      else:\n",
        "        i = i + 1\n",
        "\n",
        "    #select from C only given components\n",
        "    C_component = C.copy()\n",
        "    C_component = C_component[C_component[\"Global Component Code Description\"] == list_of_manual_components[file_number]]\n",
        "    C_component.drop_duplicates(subset = [\"Customer Requested Comment\"], inplace = True, keep = \"first\")\n",
        "    list_of_sizes_of_clean_data_sets.append(len(clean_data_set))\n",
        "\n",
        "    print(\"     \", list_of_manual_components[file_number])\n",
        "\n",
        "    #calculate confusion matrix\n",
        "\n",
        "    true_positive = 0\n",
        "    false_positive = 0\n",
        "    true_negative = 0\n",
        "    false_negative = 0\n",
        "\n",
        "    column_with_confusion_matrix_values = []\n",
        "\n",
        "    for i in range(0, len(clean_data_set)):\n",
        "      not_found = True\n",
        "      for j in range (0, len(C_component)):\n",
        "        if clean_data_set[\"Customer Requested Comment\"].iloc[i] == C_component[\"Customer Requested Comment\"].iloc[j]:\n",
        "          not_found = False\n",
        "          if clean_data_set[\"true label\"].iloc[i] == 1:\n",
        "            true_positive = true_positive + 1\n",
        "            column_with_confusion_matrix_values.append(\"TP\")\n",
        "          else:\n",
        "            false_positive = false_positive + 1\n",
        "            column_with_confusion_matrix_values.append(\"FP\")\n",
        "            #print(\"FP: \",clean_data_set[\"Customer Requested Comment\"].iloc[i])\n",
        "\n",
        "      if not_found == True:\n",
        "        if clean_data_set[\"true label\"].iloc[i] == 1:\n",
        "          false_negative = false_negative + 1\n",
        "          column_with_confusion_matrix_values.append(\"FN\")\n",
        "          #print(\"FN: \",clean_data_set[\"Customer Requested Comment\"].iloc[i])\n",
        "        else:\n",
        "          true_negative = true_negative + 1\n",
        "          column_with_confusion_matrix_values.append(\"TN\")\n",
        "\n",
        "\n",
        "    print(\"       True positive: \", true_positive)\n",
        "    print(\"       False positive: \", false_positive)\n",
        "    print(\"       True negative: \", true_negative)\n",
        "    print(\"       False negative: \", false_negative)\n",
        "\n",
        "    list_of_F1_scores.append(true_positive / (true_positive + 0.5*(false_positive + false_negative)))\n",
        "    print(\"       F1 score: \",round(list_of_F1_scores[file_number],2))\n",
        "  \n",
        "  #weighted F1 score for 6 components\n",
        "  F1_score_weighted = np.average(list_of_F1_scores, weights=list_of_sizes_of_clean_data_sets)\n",
        "  print(\"weighted F1 score: \",round(F1_score_weighted,2))\n",
        "      "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8jrZP9r-b-uZ",
        "pTOMAdxL142J",
        "GuBDPOjySF63",
        "uqHHscwO0pHx",
        "V_FmGIRPamEz",
        "He8Qs44VfXYk"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyPC1oreJvQyFEfUqZFdpNto",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}